# 3. Knowledge Configuration 

Here's a detailed and beginner-friendly explanation of the **Knowledge Configuration in CrewAI**, including real-world examples, code breakdowns, and use cases.

---

# 🧠 Understanding Knowledge Configuration in CrewAI

**CrewAI** is a powerful framework that allows AI agents to work with knowledge sources efficiently. To ensure accurate information retrieval, CrewAI offers features like **chunking**, **embedding configuration**, and **knowledge resetting**. 

These features help in processing large documents, improving retrieval accuracy, and customizing how knowledge is stored and accessed.

---

## 📌 **1. Chunking Configuration in CrewAI**
### 🔍 What is Chunking?
When working with large documents, AI models may struggle to process them efficiently. **Chunking** refers to splitting a document into smaller, manageable pieces (**chunks**) for better processing.  

### 🎯 **Why is Chunking Important?**
- ✅ **Handles Large Documents**: AI models have token limits, so breaking documents into chunks ensures smooth processing.
- ✅ **Improves Retrieval Accuracy**: When answering queries, smaller chunks provide precise context.
- ✅ **Maintains Context**: Overlapping chunks help retain information continuity.

---

### 📝 **Code Example: Chunking a String Knowledge Source**
```python
from crewai.knowledge.source.string_knowledge_source import StringKnowledgeSource

# Create a knowledge source with chunking
source = StringKnowledgeSource(
    content="Your content here",
    chunk_size=4000,      # Maximum size of each chunk (default: 4000)
    chunk_overlap=200     # Overlap between chunks (default: 200)
)
```
### 🛠 **Code Explanation**
1. **Import `StringKnowledgeSource`** – This is a module that handles string-based knowledge sources.
2. **Define `content`** – This is the text or document we want to store.
3. **Set `chunk_size=4000`** – Specifies the maximum length of each chunk.
4. **Set `chunk_overlap=200`** – Ensures overlapping context between chunks to maintain continuity.

---

### 🌍 **Real-World Use Case: Where is Chunking Used?**
📖 **Example: AI-powered Legal Document Assistant**  
A law firm uploads large legal contracts. AI processes them by breaking them into chunks for accurate retrieval when answering questions about specific clauses.

---

## 📌 **2. Embeddings Configuration**
### 🔍 **What are Embeddings?**
Embeddings are numerical representations of words and sentences that help AI understand the meaning of text.

### 🎯 **Why Configure an Embedder?**
- ✅ **Improves AI’s Understanding**: Converts text into machine-readable numerical data.
- ✅ **Optimizes Search and Retrieval**: Helps AI find relevant knowledge faster.
- ✅ **Supports Various Providers**: Choose from OpenAI, Google, Cohere, AWS Bedrock, etc.

---

### 📝 **Supported Embedding Providers**
| 🔹 Provider  | 🔹 Description |
|-------------|--------------|
| **openai** | Uses OpenAI’s embedding models |
| **google** | Uses Google’s `text-embedding-004` model |
| **azure** | Azure’s OpenAI embeddings |
| **ollama** | Local embeddings with Ollama |
| **vertexai** | Google Cloud VertexAI embeddings |
| **cohere** | Cohere’s embedding models |
| **voyageai** | VoyageAI’s embedding models |
| **bedrock** | AWS Bedrock embeddings |
| **huggingface** | Open-source Hugging Face models |
| **watson** | IBM Watson embeddings |

---

### 📝 **Code Example: Configuring Google Embeddings**
```python
from crewai import Agent, Task, Crew, Process, LLM
from crewai.knowledge.source.string_knowledge_source import StringKnowledgeSource
import os

# Get the GEMINI API key
GEMINI_API_KEY = os.environ.get("GEMINI_API_KEY")

# Create a knowledge source
content = "Users name is John. He is 30 years old and lives in San Francisco."
string_source = StringKnowledgeSource(
    content=content,
)

# Create an LLM with a temperature of 0 to ensure deterministic outputs
gemini_llm = LLM(
    model="gemini/gemini-1.5-pro-002",
    api_key=GEMINI_API_KEY,
    temperature=0,
)

# Create an agent with the knowledge store and embedding
agent = Agent(
    role="About User",
    goal="You know everything about the user.",
    backstory="""You are a master at understanding people and their preferences.""",
    verbose=True,
    allow_delegation=False,
    llm=gemini_llm,
    embedder={
        "provider": "google",
        "config": {
            "model": "models/text-embedding-004",
            "api_key": GEMINI_API_KEY,
        }
    }
)

# Define a task for the agent
task = Task(
    description="Answer the following questions about the user: {question}",
    expected_output="An answer to the question.",
    agent=agent,
)

# Create a crew with the agent, task, and knowledge source
crew = Crew(
    agents=[agent],
    tasks=[task],
    verbose=True,
    process=Process.sequential,
    knowledge_sources=[string_source],
    embedder={
        "provider": "google",
        "config": {
            "model": "models/text-embedding-004",
            "api_key": GEMINI_API_KEY,
        }
    }
)

# Execute the task
result = crew.kickoff(inputs={"question": "What city does John live in and how old is he?"})
```

---

### 🛠 **Code Explanation**
1. **Import necessary modules** – Includes `Agent`, `Task`, `Crew`, `Process`, and `LLM`.
2. **Get API key from environment** – Uses `os.environ.get("GEMINI_API_KEY")` to access the Google API key.
3. **Create a string knowledge source** – Stores the user's information.
4. **Define an LLM (Language Model)** – Uses Google's `gemini-1.5-pro-002` model.
5. **Create an AI Agent** – The agent specializes in user-related queries.
6. **Embedder Configuration** – Uses `text-embedding-004` from Google.
7. **Define a Task** – The agent is assigned to answer questions about the user.
8. **Create a Crew** – The AI team is assembled with tasks and knowledge sources.
9. **Execute the Task** – `crew.kickoff()` processes a query about John.

---

### 🌍 **Real-World Use Case: Where is Embedding Used?**
🔎 **Example: AI-Powered Resume Screening**  
A hiring platform uses **text embeddings** to compare job descriptions with resumes and find the best candidates.

---

## 📌 **3. Clearing Knowledge in CrewAI**
### 🔍 **Why Reset Knowledge?**
If you update your knowledge sources, you might need to **clear previous data** to ensure agents work with the latest information.

---

### 📝 **Command to Reset Knowledge**
```bash
crewai reset-memories --knowledge
```

### 🎯 **When to Use This?**
- ✅ After updating knowledge sources.
- ✅ When AI provides outdated answers.
- ✅ If switching between different datasets.

---

### 🌍 **Real-World Use Case: Where is Knowledge Reset Used?**
🔄 **Example: AI-Powered Customer Support**  
A chatbot trained on old FAQs can reset its knowledge and learn from the latest support documents to provide up-to-date answers.

---

# 🎯 **Conclusion**
**Knowledge configuration in CrewAI** is crucial for **efficient information retrieval, accurate AI responses, and optimized processing**. The three main components—**chunking, embeddings, and knowledge resetting**—help AI agents work effectively with different data sources.

🚀 **Key Takeaways:**
- **Chunking** breaks large content into smaller pieces for better processing.
- **Embeddings** help AI understand text better using numerical representations.
- **Resetting Knowledge** ensures AI uses the latest data.

By leveraging these features, you can **enhance AI-driven applications** in various fields like customer service, legal research, resume screening, and more! 🎉