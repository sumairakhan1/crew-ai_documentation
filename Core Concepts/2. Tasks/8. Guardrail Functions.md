# ğŸ›¡ **Understanding Guardrail Functions in CrewAI**  

When working with **AI-generated content**, itâ€™s important to **validate and filter** the outputs before using them. **Guardrail functions** help enforce **quality control** by:  
âœ… **Validating the AI-generated output** before passing it forward.  
âœ… **Ensuring structured responses** that meet requirements.  
âœ… **Providing clear error handling** for debugging.  

In this guide, weâ€™ll break down:  
âœ” **Guardrail function requirements**  
âœ” **Best practices for error handling**  
âœ” **Chaining multiple validation steps**  

---  

# ğŸ¯ **Real-World Use Case: AI-Powered Customer Support Chatbot**  

Imagine you have an **AI chatbot** that answers customer queries. The chatbot's response must:  
âœ… Be **non-empty** (to prevent blank replies).  
âœ… **Avoid inappropriate content** (filter toxic or offensive responses).  
âœ… **Follow formatting rules** (structured as a professional response).  

To achieve this, we use **guardrail functions** to validate and filter responses **before sending them to customers**.  

---

# ğŸ— **1. Guardrail Function Requirements**  

### âœ… **What is a Guardrail Function?**  
A **guardrail function** is a function that validates AI-generated outputs. It:  
- **Accepts one parameter** (the task output).  
- **Returns a tuple** of **(boolean, response)**.  
  - **`(True, validated_result)`** â†’ If the response is valid âœ…  
  - **`(False, error_details)`** â†’ If the response is invalid âŒ  
- **Uses type hints** (recommended for readability).  

---

### ğŸ–¥ **Code Example: Basic Guardrail Function**  

```python
from typing import Tuple, Union, Dict, Any

def validate_response(result: str) -> Tuple[bool, Union[str, Dict[str, Any]]]:
    """Validate AI chatbot response before sending to user."""
    
    # Check if response is empty
    if not result.strip():
        return (False, {
            "error": "Response is empty",
            "code": "EMPTY_RESPONSE"
        })

    # Check if response is too long (e.g., max 200 words)
    word_count = len(result.split())
    if word_count > 200:
        return (False, {
            "error": "Response exceeds 200 words",
            "code": "WORD_LIMIT_EXCEEDED",
            "context": {"word_count": word_count}
        })

    # If all checks pass, return valid response
    return (True, result.strip())
```

---

### ğŸ“Œ **Breaking Down the Code**  

#### ğŸ“ **Step 1: Function Definition**  
```python
def validate_response(result: str) -> Tuple[bool, Union[str, Dict[str, Any]]]:
```
- **Defines a function** `validate_response` that **takes one parameter** (`result` - the chatbot's response).  
- **Returns a tuple**:  
  - `True, validated_result` â†’ If valid âœ…  
  - `False, error_details` â†’ If invalid âŒ  

---

#### ğŸ“ **Step 2: Check for Empty Responses**  
```python
if not result.strip():
    return (False, {
        "error": "Response is empty",
        "code": "EMPTY_RESPONSE"
    })
```
- If the **response is empty or contains only spaces**, itâ€™s **invalid** âŒ.  
- **Returns an error** `"EMPTY_RESPONSE"` with details.  

---

#### ğŸ“ **Step 3: Check for Word Count Limit**  
```python
word_count = len(result.split())
if word_count > 200:
    return (False, {
        "error": "Response exceeds 200 words",
        "code": "WORD_LIMIT_EXCEEDED",
        "context": {"word_count": word_count}
    })
```
- **Splits the response** into words and **counts them**.  
- If the **word count exceeds 200**, it **returns an error** `"WORD_LIMIT_EXCEEDED"`.  
- The `"context"` field includes the **actual word count** for debugging.  

---

#### ğŸ“ **Step 4: Return Valid Response**  
```python
return (True, result.strip())
```
- If the response passes **all checks**, it **returns as valid** âœ….  

---

# âš  **2. Best Practices for Error Handling**  

### âœ… **1. Use Structured Error Responses**  
Errors should:  
âœ” Include **error codes** for easy debugging.  
âœ” Provide **context** (e.g., word count, invalid content).  
âœ” Give **clear feedback** on what went wrong.  

---

### ğŸ–¥ **Code Example: Handling Multiple Error Types**  

```python
def validate_with_context(result: str) -> Tuple[bool, Union[Dict[str, Any], str]]:
    try:
        # Main validation logic
        validated_data = perform_validation(result)
        return (True, validated_data)
    
    except ValidationError as e:
        return (False, {
            "error": str(e),
            "code": "VALIDATION_ERROR",
            "context": {"input": result}
        })
    
    except Exception as e:
        return (False, {
            "error": "Unexpected error",
            "code": "SYSTEM_ERROR"
        })
```

---

### ğŸ“Œ **Breaking Down the Code**  

#### ğŸ“ **Try Block: Main Validation Logic**  
```python
validated_data = perform_validation(result)
return (True, validated_data)
```
- Tries to **validate** the AI output using `perform_validation()`.  
- If **successful**, it returns the **validated response** âœ….  

---

#### ğŸ“ **Handling Validation Errors**  
```python
except ValidationError as e:
    return (False, {
        "error": str(e),
        "code": "VALIDATION_ERROR",
        "context": {"input": result}
    })
```
- If the **validation fails**, it returns:  
  - **Error message** (`error`).  
  - **Error code** (`VALIDATION_ERROR`).  
  - **Context** (original input for debugging).  

---

#### ğŸ“ **Handling Unexpected Errors**  
```python
except Exception as e:
    return (False, {
        "error": "Unexpected error",
        "code": "SYSTEM_ERROR"
    })
```
- If **any unknown error occurs**, it returns a `"SYSTEM_ERROR"`.  

---

# ğŸ”— **3. Chaining Multiple Validation Steps**  

### âœ… **Why Use a Validation Chain?**  
âœ” Allows **multiple validation steps** to **process outputs** step by step.  
âœ” If one step **fails**, the process **stops immediately**.  
âœ” Ensures **data is fully validated** before being used.  

---

### ğŸ–¥ **Code Example: Multi-Step Validation Chain**  

```python
from typing import Any, Dict, Tuple, Union

def complex_validation(result: str) -> Tuple[bool, Union[str, Dict[str, Any]]]:
    """Chain multiple validation steps."""
    
    # Step 1: Check if input is empty
    if not result:
        return (False, {"error": "Empty result", "code": "EMPTY_INPUT"})

    # Step 2: Validate content
    try:
        validated = validate_content(result)
        if not validated:
            return (False, {"error": "Invalid content", "code": "CONTENT_ERROR"})

        # Step 3: Format validation
        formatted = format_output(validated)
        return (True, formatted)

    except Exception as e:
        return (False, {
            "error": str(e),
            "code": "VALIDATION_ERROR",
            "context": {"step": "content_validation"}
        })
```

---

### ğŸ“Œ **Breaking Down the Code**  

#### ğŸ“ **Step 1: Check for Empty Input**  
```python
if not result:
    return (False, {"error": "Empty result", "code": "EMPTY_INPUT"})
```
- Ensures **input isnâ€™t empty** before processing.  

---

#### ğŸ“ **Step 2: Validate Content**  
```python
validated = validate_content(result)
if not validated:
    return (False, {"error": "Invalid content", "code": "CONTENT_ERROR"})
```
- Uses `validate_content()` to **check if the content is appropriate**.  

---

#### ğŸ“ **Step 3: Format Validation**  
```python
formatted = format_output(validated)
return (True, formatted)
```
- **Formats** the validated response **before passing it forward**.  

---

# ğŸ¯ **Key Takeaways**  

âœ… **Guardrail functions** ensure AI-generated outputs are **valid and high-quality**.  
âœ… **Error handling best practices** include **structured responses and debugging context**.  
âœ… **Validation chains** allow multiple **steps** to check **different aspects** of the output.  

Would you like to **extend this with real-time monitoring** or **add logging for debugging**? ğŸš€